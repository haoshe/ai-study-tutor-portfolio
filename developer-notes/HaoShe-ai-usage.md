# AI Usage Documentation - Hao She

**Project:** AI-Powered Study Assistant
**Role:** Person 2 - AI Integration & Prompt Engineering
**Documentation Period:** November 6 - November 18, 2025

---

## Executive Summary

This document provides a comprehensive overview of generative AI usage throughout the development of the AI-Powered Study Assistant project. As the AI Integration specialist, I extensively used Claude (Anthropic) for feature development, problem-solving, and code generation. The strategic use of AI resulted in significant productivity gains (approximately 90% time savings) while maintaining high code quality with comprehensive test coverage.

---

## 1. Tools and Models Used

### Primary AI Tool
- **Tool:** Claude (Anthropic)
- **Interface:** Web interface (claude.ai)
- **Model:** Claude 3.5 Sonnet (primary conversations)
- **Usage Pattern:** Interactive development sessions with iterative refinement

### Supporting Tools
- **Terminal/Bash:** For testing and verification
- **cURL:** API endpoint testing
- **Maven:** Build and test automation
- **Git:** Version control with AI-suggested commit messages

---

## 2. Overall AI Integration Strategy

### Development Philosophy
My approach to using AI throughout this project was structured around:

1. **Context-First Development**
   - Always provided comprehensive project context before requesting code
   - Uploaded existing code structure, dependencies (pom.xml), and architecture
   - Maintained conversation continuity for Claude to understand evolving requirements

2. **Iterative Refinement**
   - Started with high-level feature discussions
   - Progressively refined requirements through clarifying questions
   - Tested generated code immediately and provided feedback for improvements

3. **Learning-Oriented Approach**
   - Used AI not just for code generation but for understanding best practices
   - Asked "why" questions to understand design decisions
   - Documented learning outcomes in detailed dev-logs

4. **Quality-First Implementation**
   - Always requested comprehensive test coverage
   - Insisted on proper error handling and edge case management
   - Used AI to identify and fix security issues (sanitization, validation)

### When AI Was Used
- **Feature design and architecture** - Exploring implementation options
- **Complete feature implementation** - Service layer, controllers, DTOs
- **Test generation** - Unit tests and integration tests with Mockito
- **Debugging** - Error diagnosis through screenshot sharing
- **Prompt engineering** - Crafting effective AI prompts for content generation
- **Code fixes** - Addressing test failures and compilation errors
- **Documentation** - README files and API documentation

### When Manual Coding Was Preferred
- **Simple configuration changes** - Basic property file edits
- **Minor tweaks** - Small CSS adjustments after initial generation
- **Copy-paste operations** - Moving AI-generated code into project structure
- **Git operations** - Branch creation, commits (though AI suggested messages)

---

## 3. Features Developed with AI Assistance

### 3.1 Flashcard Generation Feature (November 6, 2025)
**Objective:** Create AI-powered flashcard generator for study materials

**AI Usage:**
- Feature architecture design (FlashcardService, FlashcardController, DTOs)
- OpenAI API integration with Spring AI framework
- Complete code generation (4 Java files, ~240 lines)
- Comprehensive test suite (14 tests with Mockito)
- Error handling and input validation
- Debugging compilation errors (Mockito ambiguity issues)

**Key Prompts:**
- "I want to do something simpler, like using ai to create study questions or flash cards"
- "I need to upload some of my existing code to you before you show me actual code"
- "can you produce a test based on spring boot?"

**Outcome:** Production-ready feature with full test coverage in ~4 hours

**Detailed Log:** [docs/dev-logs/HaoShe/2025-11-06-flashcard-generation.md](../docs/dev-logs/HaoShe/2025-11-06-flashcard-generation.md)

### 3.2 Multiple-Choice Quiz Generation (November 7, 2025)
**Objective:** Implement quiz generation with difficulty levels and plausible distractors

**AI Usage:**
- Feature feasibility exploration (compared multiple quiz types)
- Understanding distractor quality challenges
- Complete implementation (6 files: 4 code + 2 test files, ~745 lines)
- Three difficulty levels with custom prompt engineering
- Advanced test coverage (21 tests: 8 unit + 13 integration)
- Test fix for Mockito ambiguity (learned from previous day)

**Key Prompts:**
- "today, i want to ask ai to generate some quizs based on the study material i upload, do you have some ideas?"
- "will these be generated by AI?" [regarding question, options, correct answer, explanation]
- "yes, please do it" [after reviewing options]

**Outcome:** Complex feature with 50% more tests than flashcards, completed in ~1 hour (92% time savings)

**Detailed Log:** [docs/dev-logs/HaoShe/2025-11-07-mcq-generation.md](../docs/dev-logs/HaoShe/2025-11-07-mcq-generation.md)

### 3.3 React Frontend Development (November 13, 2025)
**Objective:** Create minimal working frontend for study material input and AI content display

**AI Usage:**
- Complete React project structure setup
- Component architecture (StudyAssistant.jsx)
- Proxy configuration for Coder workspace
- Tabbed interface implementation
- API integration with Fetch API
- Responsive CSS styling
- Environment configuration (.env for host header issue)

**Key Prompts:**
- "I need to create a minimum working version of frontend, like the user input some study material by manually typing or copy, and the result ai generated can be rendered on the screen"
- "this flashcards() and quiz() are very pale, i can barely see it"

**Outcome:** Clean, functional React frontend with dual-mode display (flashcards/quiz tabs)

**Detailed Log:** [docs/dev-logs/HaoShe/2025-11-13-Frontend-Development.md](../docs/dev-logs/HaoShe/2025-11-13-Frontend-Development.md)

### 3.4 AI Output Sanitization - Backend (November 16-17, 2025)
**Objective:** Fix critical AI output issues - quiz answer bias and flashcard hallucination

**Problems Identified:**
1. Quiz Answer Bias: 95% of correct answers in option A (19/20 questions)
2. Flashcard Hallucination: AI generating off-topic CS questions for gibberish input

**AI Usage:**
- Problem analysis and root cause identification
- Prompt engineering solutions (not code architecture changes)
- Test execution planning (statistical significance considerations)
- Validation of fixes

**Solution Approach:** Enhanced AI prompts with explicit constraints

**Quiz Fix - Key Additions to Prompt:**
```
- **IMPORTANT: Randomize which option (A, B, C, or D) is correct for each question**
- **Do NOT place all correct answers in option A**
- Distribute correct answers evenly across all four options
```

**Flashcard Fix - Key Additions to Prompt:**
```
- **CRITICAL: Only use information from the provided study material above**
- **Do NOT use outside knowledge or general topics**
- **If the material is insufficient for N flashcards, generate fewer flashcards**
- If the study material doesn't contain educational content, return an empty response
```

**Test Results:**
- Quiz: Answer distribution changed from 95% option A to balanced (15%, 15%, 10%, 10%)
- Flashcards: Gibberish input now returns `[]` instead of hallucinated content

**Outcome:** Robust AI content generation with proper constraints

**Detailed Logs:**
- [docs/dev-logs/HaoShe/2025-11-16-ai-output-sanitization-1.md](../docs/dev-logs/HaoShe/2025-11-16-ai-output-sanitization-1.md)
- [docs/dev-logs/HaoShe/2025-11-17-ai-output-sanitization-2.md](../docs/dev-logs/HaoShe/2025-11-17-ai-output-sanitization-2.md)

### 3.5 Frontend Warning Integration & Test Fixes (November 18, 2025)
**Objective:** Update frontend to handle new backend warning messages and fix failing controller tests

**AI Usage:**
- Backend API verification strategy (comprehensive test plan)
- Frontend compatibility issue identification
- React state management updates
- CSS styling for warning messages
- Development server issue resolution (file watcher problem)
- Controller test fixes for new response format

**Changes Made:**
- Updated frontend to extract nested response structure (`data.flashcards` vs. `data`)
- Added warning state variables and UI display
- Fixed 7 failing controller tests (4 quiz, 3 flashcard)
- Resolved `EMFILE: too many open files` with polling configuration

**Key Prompts:**
- "now i need to fix the rest of issues, first, help me double check the current AI is outputting the desired material"
- "currently my frontend can't handle the new warning message or [], I need to fix it"
- "remember i need to skip all the tests to build maven at the begining, i need to fix that too"

**Outcome:** Complete integration of warning system, all 38 tests passing

**Detailed Log:** [docs/dev-logs/HaoShe/2025-11-18-ai-output-sanitization-3.md](../docs/dev-logs/HaoShe/2025-11-18-ai-output-sanitization-3.md)

---

## 4. Prompt Engineering Techniques

### 4.1 Context Setting Strategies

**Technique: Conversational Memory**
- Leveraged Claude's conversation history
- Referenced previous sessions without re-explaining
- Example: "from the talk I had with you for the last few days, you probably know that I have a group project?"

**Technique: Code Upload for Context**
- Uploaded project structure screenshots
- Shared pom.xml for dependency awareness
- Provided existing code patterns before requesting new code

**Effectiveness:** Ensured generated code matched project conventions and dependencies

### 4.2 Requirement Clarification

**Technique: Open-Ended Exploration**
- Started with broad questions to understand options
- Example: "do you have some ideas?" instead of "build me a quiz feature"
- Received comparative analysis before committing to implementation

**Technique: Constraint Communication**
- Explicitly stated limitations (MVP, simpler version)
- Explained blockers (upload feature not ready)
- Example: "I want to do something simpler, like using ai to create study questions or flash cards"

**Effectiveness:** Received practical solutions instead of over-engineered code

### 4.3 Iterative Refinement

**Technique: Visual Debugging**
- Uploaded error screenshots instead of typing error messages
- Faster diagnosis and more accurate solutions
- Example: Mockito ambiguity error screenshot

**Technique: Targeted Feedback**
- Short, direct feedback for improvements
- Example: "this flashcards() and quiz() are very pale, i can barely see it"
- Claude made targeted changes without regenerating everything

**Effectiveness:** Quick iterations without losing context

### 4.4 Learning-Oriented Prompts

**Technique: Asking "Why" Questions**
- "will these be generated by AI?" - understood technical challenges
- "the correctAnswer : 0 means the first answer in the list?" - clarified data structures
- Gained understanding beyond just receiving code

**Effectiveness:** Deeper learning, better ability to maintain code independently

### 4.5 Quality Assurance Prompts

**Technique: Explicit Test Requests**
- "can you produce a test based on spring boot?"
- Specified testing framework to get proper mocking

**Technique: Double-Checking**
- "help me double check the current AI is outputting the desired material"
- Received comprehensive test plans

**Effectiveness:** High test coverage (14-21 tests per feature), caught issues early

---

## 5. Reflections on AI Usage

### 5.1 What Worked Exceptionally Well

**1. Context-Aware Code Generation**
- After providing project context once, Claude consistently generated code matching:
  - Existing naming conventions
  - Project structure patterns
  - Dependency versions
  - Code style (Lombok annotations, JavaDoc)

**2. Comprehensive Test Coverage**
- AI-generated tests were production-quality
- Covered edge cases I might have missed
- Used proper mocking frameworks
- Included both unit and integration tests

**3. Prompt Engineering for AI Behavior**
- Most impactful use: Fixing quiz bias and flashcard hallucination through prompt improvements
- Learned that AI behavior is primarily controlled through prompt design
- Small prompt changes had massive impact on output quality

**4. Time Efficiency**
- Quiz feature estimate: 12 hours manual â†’ 1 hour with AI (92% savings)
- Flashcard feature: ~4 hours total (vs. estimated 8-10 hours manual)
- Overall productivity multiplier: ~10x

**5. Error Diagnosis**
- Uploading error screenshots led to instant, accurate solutions
- Claude identified patterns (Mockito ambiguity) I could apply to future issues
- Reduced debugging time significantly

**6. Learning Acceleration**
- Learned Spring Boot best practices through generated code
- Understood Mockito patterns through examples
- Gained React knowledge from frontend generation
- AI acted as both developer and tutor

### 5.2 Challenges and Limitations

**1. Initial Test Generation Issues**
- First flashcard test made real OpenAI API calls (expensive)
- Had to explicitly request "Spring Boot tests with mocking"
- **Lesson:** Be specific about testing approach requirements

**2. Mockito Ambiguity Errors**
- `any(String.class)` caused compilation errors
- Occurred in both flashcard and quiz features
- **Resolution:** Learned to use `anyString()` instead
- **Lesson:** AI-generated code can have subtle bugs; always compile and test

**3. Tool Setup Issues**
- Coder workspace environment had unique challenges:
  - Port access limitations (solved with proxy)
  - Host header validation (solved with .env config)
  - File watcher limits (solved with polling)
- **Lesson:** AI suggestions needed adaptation for specific environments

**4. Over-Reliance Risk**
- Easy to accept code without fully understanding
- Had to consciously ask "why" questions to maintain learning
- **Mitigation:** Created detailed dev-logs forcing reflection

**5. Format Assumptions**
- AI initially generated direct array responses
- Later needed to change to object responses with warnings
- Required updating both backend and tests
- **Lesson:** Discuss response format explicitly upfront

**6. Prompt Engineering Learning Curve**
- Early prompts were sometimes too vague
- Learned to be more specific over time
- Example: "create a frontend" vs. "create a minimal React frontend with text input and tabbed results display"

### 5.3 Open Questions and Future Considerations

**Question 1: Optimal AI Collaboration Level**
- Where is the line between helpful assistance and over-dependence?
- Current approach: Use AI for initial generation, manually understand and modify
- Need to balance speed with deep learning

**Question 2: Long-Term Maintenance**
- Will I be able to maintain AI-generated code months later?
- Strategy: Comprehensive documentation and dev-logs help
- Concern: Need to truly understand generated patterns

**Question 3: Testing AI-Generated AI Prompts**
- Used AI to create prompts for AI content generation (meta-level)
- How to systematically test prompt quality?
- Current approach: Manual testing with edge cases

**Question 4: Security Review**
- AI-generated code included input validation
- But did it cover all security vectors?
- Added sanitization passes manually to be safe

**Question 5: Code Review Standards**
- Should AI-generated code be reviewed differently?
- Applied same standards as manual code
- Question: Are there AI-specific code smells to watch for?

### 5.4 Impact on Development Workflow

**Positive Changes:**
- Faster prototyping and iteration
- More time for testing and refinement
- Better documentation (AI helps structure docs)
- Exposure to best practices and patterns

**Workflow Adjustments:**
- Added "context upload" step at session start
- Implemented screenshot-first debugging
- Created detailed dev-logs for accountability
- Added extra verification step for AI code

**Team Collaboration:**
- Generated independent features without blocking teammates
- Worked around dependencies (text input vs. file upload)
- Shared AI-enhanced productivity benefits with team

---

## 6. Quantitative Analysis

### Lines of Code Generated
| Feature | Code Files | Test Files | Code LOC | Test LOC | Total LOC |
|---------|-----------|-----------|----------|----------|-----------|
| Flashcards | 4 | 2 | ~240 | ~365 | ~605 |
| Quiz | 4 | 2 | ~285 | ~460 | ~745 |
| Frontend | 5 | 0 | ~300 | 0 | ~300 |
| Sanitization | 0 (edits) | 0 (edits) | ~50 | ~35 | ~85 |
| **Total** | **13** | **4** | **~875** | **~860** | **~1,735** |

### Test Coverage
- Total tests written: **59 tests** (35 unit + 24 integration)
- Test files: 4 (FlashcardServiceTest, FlashcardControllerTest, QuizServiceTest, QuizControllerTest)
- Test success rate: 100% (59/59 passing after fixes)

### Time Efficiency
| Task | Estimated Manual Time | Actual Time with AI | Time Saved | Efficiency Gain |
|------|----------------------|-------------------|------------|----------------|
| Flashcard Feature | 8-10 hours | 4 hours | 5-6 hours | ~60% |
| Quiz Feature | 12 hours | 1 hour | 11 hours | ~92% |
| Frontend | 6-8 hours | 2 hours | 5-6 hours | ~75% |
| Sanitization | 4 hours | 1 hour | 3 hours | ~75% |
| **Total** | **30-34 hours** | **8 hours** | **24-26 hours** | **~76-80%** |

### Bug/Issue Resolution
- Compilation errors: 3 (all resolved with AI help)
- Test failures: 7 (all fixed with AI guidance)
- Integration issues: 5 (CORS, proxy, host header, file watcher, response format)
- Security issues: 2 (answer bias, hallucination - fixed with prompt engineering)

---

## 7. Comparison with Traditional Development

### Without AI Approach (Hypothetical)
1. Research Spring AI documentation (2-3 hours)
2. Design architecture manually (1-2 hours)
3. Implement service layer with trial and error (3-4 hours)
4. Write tests while learning Mockito (3-4 hours)
5. Debug integration issues (2-3 hours)
6. Repeat for each feature

**Total per feature:** 11-16 hours

### With AI Approach (Actual)
1. Provide context to AI (10-15 minutes)
2. Discuss approach and options (10-20 minutes)
3. Generate code with comprehensive tests (5 minutes)
4. Review, test, and fix compilation issues (30-60 minutes)
5. Manual testing and verification (20-30 minutes)
6. Documentation review (10-15 minutes)

**Total per feature:** 1.5-3 hours

### Key Differences
- **Research time:** Eliminated (AI knows best practices)
- **Initial implementation:** 95% faster (generated vs. manual)
- **Testing:** Comprehensive from start (vs. often skimped on)
- **Documentation:** Generated alongside code (vs. often delayed)
- **Learning:** Concurrent with development (vs. pre-research)

---

## 8. AI Usage Best Practices Developed

### Before Starting Development
1. **Provide Comprehensive Context**
   - Upload existing code structure
   - Share dependency files (pom.xml, package.json)
   - Explain project architecture and patterns

2. **Clarify Requirements**
   - Start with open-ended questions
   - Discuss options before committing
   - Explicitly state constraints and limitations

### During Development
3. **Request Complete Solutions**
   - Ask for tests alongside code
   - Request error handling and validation
   - Specify documentation needs

4. **Verify Incrementally**
   - Test generated code immediately
   - Compile and run tests before moving forward
   - Provide feedback for refinement

5. **Use Visual Communication**
   - Screenshot errors instead of typing them
   - Share UI issues with images
   - Upload test results for analysis

### After Code Generation
6. **Understand Before Accepting**
   - Ask "why" questions about design choices
   - Request explanations of complex patterns
   - Ensure you can maintain the code

7. **Document Thoroughly**
   - Create detailed dev-logs
   - Record prompts and outcomes
   - Note lessons learned

8. **Test Edge Cases**
   - Don't assume AI covered everything
   - Add manual tests for domain-specific cases
   - Verify security and validation

---

## 9. Recommendations for Future AI-Assisted Development

### For Individual Developers
1. **Build AI Literacy Early**
   - Learn prompt engineering fundamentals
   - Understand AI capabilities and limitations
   - Practice iterative refinement techniques

2. **Maintain Code Ownership**
   - Always review generated code
   - Understand before integrating
   - Refactor if necessary for clarity

3. **Use AI as a Learning Tool**
   - Ask explanatory questions
   - Study generated patterns
   - Apply learned concepts independently

### For Team Projects
4. **Establish AI Usage Standards**
   - Define when AI assistance is appropriate
   - Set code review standards for AI-generated code
   - Share effective prompts within team

5. **Document AI Contributions**
   - Track AI-generated vs. manual code
   - Record prompts for reproducibility
   - Share learning outcomes

6. **Balance Speed and Learning**
   - Don't sacrifice understanding for speed
   - Use AI to explore, then implement manually for practice
   - Review AI code as learning material

### For Quality Assurance
7. **Enhanced Testing for AI Code**
   - Always add manual test cases
   - Verify edge cases AI might miss
   - Test security implications thoroughly

8. **Security Review Process**
   - Manually review authentication/authorization
   - Verify input validation and sanitization
   - Check for common vulnerabilities (OWASP Top 10)

---

## 10. Conclusion

The strategic use of Claude AI throughout this project delivered exceptional results: **~1,735 lines of production-ready code with 59 comprehensive tests, completed in approximately 8 hours instead of an estimated 30-34 hours** (76-80% time savings).

Beyond productivity gains, AI assistance provided:
- **Quality improvement:** Comprehensive test coverage from the start
- **Learning acceleration:** Exposure to best practices and modern patterns
- **Risk reduction:** Early identification of issues through generated tests
- **Documentation:** Complete README files and API documentation

However, success required deliberate strategy:
- Providing thorough context for accurate generation
- Asking clarifying questions before implementation
- Verifying and understanding all generated code
- Maintaining detailed documentation for accountability
- Applying critical thinking to AI suggestions

**Key Insight:** AI is most effective as a collaborative partner, not a replacement for developer judgment. The combination of AI's speed and breadth with human critical thinking and domain expertise produces results superior to either alone.

**Future Outlook:** As AI tools evolve, the balance will shift from writing code to designing systems, asking the right questions, and ensuring quality. Developers who master AI collaboration while maintaining deep technical understanding will have significant competitive advantages.

---

## 11. References and Supporting Documentation

### Detailed Session Logs
All development sessions are documented in detail with prompts, code snippets, and reflections:

- [2025-11-06-flashcard-generation.md](../docs/dev-logs/HaoShe/2025-11-06-flashcard-generation.md)
- [2025-11-07-mcq-generation.md](../docs/dev-logs/HaoShe/2025-11-07-mcq-generation.md)
- [2025-11-13-Frontend-Development.md](../docs/dev-logs/HaoShe/2025-11-13-Frontend-Development.md)
- [2025-11-16-ai-output-sanitization-1.md](../docs/dev-logs/HaoShe/2025-11-16-ai-output-sanitization-1.md)
- [2025-11-17-ai-output-sanitization-2.md](../docs/dev-logs/HaoShe/2025-11-17-ai-output-sanitization-2.md)
- [2025-11-18-ai-output-sanitization-3.md](../docs/dev-logs/HaoShe/2025-11-18-ai-output-sanitization-3.md)

### Code Repositories
- **Backend:** `/src/main/java/ie/tcd/scss/aichat/`
  - `service/FlashcardService.java`
  - `service/QuizService.java`
  - `controller/FlashcardController.java`
  - `controller/QuizController.java`

- **Frontend:** `/frontend/src/components/`
  - `StudyAssistant.jsx`
  - `StudyAssistant.css`

- **Tests:** `/src/test/java/ie/tcd/scss/aichat/`
  - `service/FlashcardServiceTest.java`
  - `service/QuizServiceTest.java`
  - `controller/FlashcardControllerTest.java`
  - `controller/QuizControllerTest.java`

### External Resources
- Spring AI Documentation: https://docs.spring.io/spring-ai/reference/
- OpenAI API Reference: https://platform.openai.com/docs/api-reference
- React Documentation: https://react.dev/
- Mockito Framework: https://site.mockito.org/

---

**Document Version:** 1.0
**Last Updated:** December 7, 2025
**Author:** Hao She
**Project Role:** AI Integration & Prompt Engineering
