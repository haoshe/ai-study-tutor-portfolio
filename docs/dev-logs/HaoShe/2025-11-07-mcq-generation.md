# AI-Assisted Development Log: Multiple-Choice Quiz Generator

**Developer:** Hao (AI Integration)  
**Date:** November 7, 2025  
**Feature:** AI-Powered Multiple-Choice Quiz Generation  
**AI Assistant:** Claude (Anthropic)

---

## Session Overview

This log documents how I used Claude AI to design and implement a multiple-choice quiz generation feature for our AI-Powered Study Assistant project. This builds on yesterday's flashcard feature and adds assessment capabilities with difficulty levels.

---

## 1. Feature Conceptualization

### My Prompt:
> "today, i want to ask ai to generate some quizs based on the study material i upload, do you have some ideas?"

### Claude's Response:
Claude provided three options:
1. Multiple Choice Quiz (recommended for MVP)
2. True/False Questions
3. Fill-in-the-Blank

Claude explained that **multiple-choice is best** because:
- Most versatile and useful
- Can reuse flashcard architecture
- Easy to demo
- Industry standard for e-learning

**Key Learning:** Starting with a question about feasibility helped me understand the options before committing to implementation.

---

## 2. Understanding the Technical Challenge

### My Question:
> "will these be generated by AI?" (referring to the question, options, correct answer, explanation)

### Claude's Explanation:
Claude clarified that **AI generates everything**, but the hard part is:
- Getting AI to create **plausible distractors** (wrong answers that seem right)
- Parsing complex response format (Q/A/B/C/D/CORRECT/EXPLAIN)
- Ensuring AI follows the format correctly

**Example of Good vs Bad Distractors:**

**Bad (too obvious):**
```
Q: What is Spring Boot?
A: A Java framework ✓
B: A pizza topping
C: A car brand
D: A movie
```

**Good (plausible):**
```
Q: What is Spring Boot?
A: A Java framework ✓
B: A JavaScript testing library
C: A build tool like Maven
D: A database management system
```

**Key Learning:** The main challenge isn't calling the API - it's prompt engineering to get good quality questions.

---

## 3. Code Generation Request

### My Prompt:
> "yes, please do it"

### What Claude Generated:
Claude immediately created 6 complete files:

**Main Code (4 files):**
1. QuizQuestion.java - DTO with question, options, correctAnswer, explanation
2. QuizRequest.java - Request DTO with studyMaterial, count, difficulty
3. QuizService.java - Business logic with OpenAI integration
4. QuizController.java - REST API endpoint

**Tests (2 files):**
5. QuizServiceTest.java - 8 unit tests with Mockito
6. QuizControllerTest.java - 13 integration tests with MockMvc

**Key Features Generated:**
- Three difficulty levels (easy/medium/hard)
- Smart parsing with regex + fallback
- Comprehensive input validation
- 21 total tests
- Complete error handling

**Key Learning:** Claude generated production-ready code without me specifying requirements in detail because it had context from yesterday's flashcard session.

---

## 4. Fixing Compilation Error

### Problem Encountered:
When copying QuizControllerTest.java, IntelliJ showed error:
```
The method any(Class<String>) is ambiguous for the type QuizControllerTest
```

### My Action:
Uploaded screenshot of the error

### Claude's Diagnosis:
Same issue as yesterday - `any(String.class)` is ambiguous in Mockito.

### Solution Applied:
Changed from:
```java
when(quizService.generateQuiz(any(String.class), eq(1), eq("hard")))
```

To:
```java
when(quizService.generateQuiz(anyString(), eq(1), eq("hard")))
```

**Applied to 6 locations** in the test file.

**Key Learning:** `anyString()` is Mockito best practice for String parameters. Learned this pattern from yesterday and could identify it faster today.

---

## 5. Testing the Implementation

### Running Tests:
```bash
mvn test -Dtest=QuizServiceTest,QuizControllerTest
```

**Results:**
- ✅ QuizServiceTest: 8/8 passed
- ✅ QuizControllerTest: 13/13 passed
- ✅ Total: 21/21 tests passing

### Testing the API:
```bash
curl -X POST http://localhost:8080/api/quiz/generate \
  -H "Content-Type: application/json" \
  -d '{
    "studyMaterial": "Spring Boot is an open-source Java framework...",
    "count": 3,
    "difficulty": "medium"
  }' | python3 -m json.tool
```

**Sample Response:**
```json
[
  {
    "question": "Which annotation is commonly used in Spring Boot to inject a bean dependency?",
    "options": [
      "@Autowired",
      "@Component",
      "@Bean",
      "@Service"
    ],
    "correctAnswer": 0,
    "explanation": "In Spring Boot, @Autowired marks a dependency for automatic injection..."
  }
]
```

**Key Learning:** The API worked on first try because tests ensured everything was correctly integrated.

---

## 6. Understanding the Output Format

### My Question:
> "the correctAnswer : 0 means the first answer in the list?"

### Clarification:
Yes - zero-indexed array:
- `correctAnswer: 0` = First option (A)
- `correctAnswer: 1` = Second option (B)
- `correctAnswer: 2` = Third option (C)
- `correctAnswer: 3` = Fourth option (D)

**Key Learning:** Understanding data structures is important for frontend integration. Using array indices is standard practice in APIs.

---

## 7. Commit Message

### My Question:
> "what git commit message i should write?"

### Claude's Recommendation:
```
feat: add AI-powered multiple-choice quiz generation

- Add QuizService with OpenAI integration for quiz generation
- Add REST API endpoint POST /api/quiz/generate
- Add QuizQuestion and QuizRequest DTOs
- Support 3 difficulty levels (easy/medium/hard)
- Generate 4-option multiple-choice questions with explanations
- Add comprehensive unit tests (8) and integration tests (13)
- All 21 tests passing

Implements Person 2 quiz generation feature
```

**Key Learning:** Good commit messages document what the code does, not necessarily every manual action taken.

---

## Prompt Engineering Techniques Used Today

### 1. **Open-Ended Exploration**
"do you have some ideas?"
- Got comparative analysis of options
- Made informed decision before coding

### 2. **Clarifying Questions**
"will these be generated by AI?"
- Understood the technical challenge
- Learned about distractors and prompt engineering

### 3. **Direct Requests**
"yes, please do it"
- Clear go-ahead for implementation
- Leveraged existing context from yesterday

### 4. **Visual Debugging**
Uploaded error screenshot
- Faster than describing the problem
- Got immediate, accurate solution

### 5. **Conceptual Questions**
"the correctAnswer : 0 means the first answer?"
- Clarified data structures
- Better understanding for frontend work

---

## Code Quality Achieved

### Generated Code Includes:
✅ Multiple-choice question generation (4 options)  
✅ Three difficulty levels with custom prompts  
✅ Smart parsing (regex primary, line-by-line fallback)  
✅ Comprehensive input validation  
✅ Error handling for edge cases  
✅ 21 comprehensive tests (8 unit + 13 integration)  
✅ JavaDoc documentation  
✅ RESTful API design  
✅ Proper HTTP status codes  

### Complexity Comparison:
- **Flashcards (yesterday):** ~605 lines, 14 tests
- **Quiz (today):** ~745 lines, 21 tests
- **Increase:** +140 lines (23% more), +7 tests (50% more)

**Why more complex?**
- More fields per question (4 options vs 1 answer)
- Difficulty level parameter
- More complex parsing (7 parts vs 2 parts)
- More edge cases to test

---

## What I Learned About AI Integration

### Technical Insights:
1. **Prompt Engineering is Critical** - The quality of quiz questions depends on prompt design
2. **Fallback Strategies** - Always have backup parsing methods
3. **Difficulty Tuning** - Achieved through prompt instructions, not separate AI calls
4. **Testing Patterns** - Similar test structure to flashcards (mock, assert, verify)

### Process Improvements:
1. **Faster Debug Cycle** - Recognized the `any()` error pattern immediately
2. **Better Context Setting** - Claude remembered yesterday's flashcard structure
3. **Incremental Testing** - Tests before manual API testing
4. **Documentation First** - README files generated alongside code

---

## Time Efficiency Analysis

### Estimated Time Without AI:
- Research multiple-choice generation: 2 hours
- Design data structures: 1 hour
- Implement service logic: 3 hours
- Implement controller: 1 hour
- Write 21 tests: 3 hours
- Debug and fix issues: 2 hours
**Total: ~12 hours**

### Actual Time With AI:
- Discussion and design: 15 minutes
- Code generation: 5 minutes
- Copy files and fix errors: 20 minutes
- Run tests: 2 minutes
- Manual API testing: 10 minutes
- Documentation review: 10 minutes
**Total: ~1 hour**

### Efficiency Gain: ~92% time savings

---

## Deliverables from Today's Session

### Code Files:
- ✅ QuizController.java (~70 lines)
- ✅ QuizService.java (~180 lines)
- ✅ QuizQuestion.java (~20 lines)
- ✅ QuizRequest.java (~15 lines)
- ✅ QuizServiceTest.java (~220 lines)
- ✅ QuizControllerTest.java (~240 lines)

### Documentation:
- ✅ QUIZ_README.md (comprehensive guide)
- ✅ QUIZ_QUICK_START.md (fast reference)
- ✅ QUIZ_FILE_SUMMARY.md (file listing)

### API Endpoints:
- ✅ POST /api/quiz/generate
- ✅ GET /api/quiz/test

### Status:
- ✅ All 21 tests passing
- ✅ API functional and tested
- ✅ Ready for demo
- ✅ Ready for file upload integration

---

## Key Differences from Yesterday's Session

| Aspect | Flashcards (Nov 6) | Quiz (Nov 7) |
|--------|-------------------|-------------|
| **Initial Prompt** | Specific (flashcards) | Exploratory (ideas?) |
| **Decision Making** | Straightforward | Compared options first |
| **Complexity** | Simple Q&A | Complex multi-option |
| **Errors** | One (Mockito ambiguity) | One (same Mockito issue) |
| **Resolution Time** | ~30 min | ~10 min (learned pattern) |
| **Development Time** | 4 hours | 1 hour |
| **Tests Generated** | 14 | 21 |

---

## Technical Challenge: Prompt Engineering

### The Difficulty Prompt System

**Easy Difficulty:**
```
Questions should test basic recall and recognition of facts
```

**Medium Difficulty (Default):**
```
Questions should test understanding and application of concepts
```

**Hard Difficulty:**
```
Questions should require analysis, evaluation, and deep understanding
```

### The Main Prompt Structure:
```
Generate N multiple-choice quiz questions from the following study material.

Study Material:
[user content]

Requirements:
- Each question should have exactly 4 options (A, B, C, D)
- Only ONE option is correct
- Wrong answers (distractors) must be plausible but clearly incorrect
- Distractors should be related to the topic
- [Difficulty-specific instructions]
- Include a brief explanation

Format each question EXACTLY like this:
Q: [question]
A: [option A]
B: [option B]
C: [option C]
D: [option D]
CORRECT: [A/B/C/D]
EXPLAIN: [explanation]
```

**Key Learning:** Specific format instructions with examples help AI follow structure consistently.









